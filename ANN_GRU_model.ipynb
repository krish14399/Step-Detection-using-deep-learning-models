{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Group M\n",
    "\n",
    "Team Members:\n",
    "1. Akshay Augustine Sheby - 5123774\n",
    "2. Krishnapriya Krishnan Santhadevi - 5123779\n",
    "3. Megha Eldho - 5123773\n",
    "4. Ranjitha Umesh - 5123734\n",
    "\n",
    "# GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_E7aN1WU5qWy",
    "outputId": "f5b77731-9258-48e1-e36e-1cd806c41bde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0  AccelX_5  AccelY_5  AccelZ_5   GyroX_5   GyroY_5   \n",
      "0                0  1.370639  3.077730 -9.138201  0.026021 -0.025069  \\\n",
      "1                1  1.380689  3.039416 -9.200333  0.038649 -0.038450   \n",
      "2                2  1.378264  2.981465 -9.305405  0.043459 -0.038100   \n",
      "3                3  1.423814  2.944719 -9.343213  0.042548 -0.028578   \n",
      "4                4  1.422443  2.946009 -9.392369  0.027376 -0.014168   \n",
      "...            ...       ...       ...       ...       ...       ...   \n",
      "722577      722577 -0.572854  7.180082  6.513024  0.001732  0.005325   \n",
      "722578      722578 -0.538156  7.221120  6.618960  0.001231  0.003183   \n",
      "722579      722579 -0.520193  7.248638  6.627628  0.001357  0.013642   \n",
      "722580      722580 -0.527089  7.316613  6.646155  0.008508  0.025486   \n",
      "722581      722581 -0.503076  7.346765  6.589603  0.011767  0.025111   \n",
      "\n",
      "         GyroZ_5  start_step_labels  end_step_labels  \n",
      "0       0.026772                0.0              0.0  \n",
      "1       0.035676                0.0              0.0  \n",
      "2       0.031424                0.0              0.0  \n",
      "3       0.029073                0.0              0.0  \n",
      "4       0.016098                0.0              0.0  \n",
      "...          ...                ...              ...  \n",
      "722577  0.004659                0.0              0.0  \n",
      "722578  0.008544                0.0              0.0  \n",
      "722579  0.009637                0.0              0.0  \n",
      "722580  0.007495                0.0              0.0  \n",
      "722581  0.007824                0.0              0.0  \n",
      "\n",
      "[722582 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare the data\n",
    "# Load input features from CSV file\n",
    "\n",
    "file_path = 'Kaggle competition dataset/data_full_+1_-1.csv'\n",
    "datasets = pd.read_csv(file_path)\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dGT4lIq36Bbm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        ...,\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]])\n",
      "tensor([[ 1.3706e+00,  3.0777e+00, -9.1382e+00,  2.6021e-02, -2.5069e-02,\n",
      "          2.6772e-02],\n",
      "        [ 1.3807e+00,  3.0394e+00, -9.2003e+00,  3.8649e-02, -3.8450e-02,\n",
      "          3.5676e-02],\n",
      "        [ 1.3783e+00,  2.9815e+00, -9.3054e+00,  4.3459e-02, -3.8100e-02,\n",
      "          3.1424e-02],\n",
      "        ...,\n",
      "        [-5.2019e-01,  7.2486e+00,  6.6276e+00,  1.3570e-03,  1.3642e-02,\n",
      "          9.6370e-03],\n",
      "        [-5.2709e-01,  7.3166e+00,  6.6462e+00,  8.5080e-03,  2.5486e-02,\n",
      "          7.4950e-03],\n",
      "        [-5.0308e-01,  7.3468e+00,  6.5896e+00,  1.1767e-02,  2.5111e-02,\n",
      "          7.8240e-03]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convert input features and labels to PyTorch tensors\n",
    "X = torch.tensor(datasets.values[:,1:7], dtype=torch.float64)\n",
    "y = torch.tensor(datasets.values[:,7:9], dtype=torch.int64)\n",
    "print(y)\n",
    "print(X) \n",
    "\n",
    "\n",
    "# 2. Create a GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.num_layers = len(hidden_sizes)\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.GRU(input_size, hidden_sizes[0], batch_first=True))\n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.hidden_layers.append(nn.GRU(hidden_sizes[i], hidden_sizes[i+1], batch_first=True))\n",
    "        self.fc = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.hidden_layers[0](x)\n",
    "        for i in range(1, self.num_layers):\n",
    "            output, _ = self.hidden_layers[i](output)\n",
    "        output = self.fc(output)  # Adjusted indexing\n",
    "        #output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 3. Set hyperparameters and initialize the model\n",
    "input_size = 6\n",
    "hidden_sizes = [32, 16, 8]\n",
    "output_size = 2  # 0 or 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Create the model and move it to the specified device\n",
    "model = GRUModel(input_size, hidden_sizes, output_size).to(device)\n",
    "model.to(torch.float64) #ensures that the data type of the model's parameters matches the data type of your input features\n",
    "\n",
    "# Move the input features and labels to the specified device\n",
    "X = X.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sfi4rm5MBZii",
    "outputId": "ac7f063c-1987-4fb1-be7a-7bd53fbae11b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 12.1955, Train Accuracy: 63.37253073007631\n",
      "Epoch [2/10], Train Loss: 11.8946, Train Accuracy: 66.05707033942168\n",
      "Epoch [3/10], Train Loss: 11.8009, Train Accuracy: 64.84364404316742\n",
      "Epoch [4/10], Train Loss: 11.7393, Train Accuracy: 63.67491855595628\n",
      "Epoch [5/10], Train Loss: 11.6964, Train Accuracy: 64.67854167416294\n",
      "Epoch [6/10], Train Loss: 11.6705, Train Accuracy: 64.44984790653517\n",
      "Epoch [7/10], Train Loss: 11.6448, Train Accuracy: 63.004959990700016\n",
      "Epoch [8/10], Train Loss: 11.6262, Train Accuracy: 62.030745299495415\n"
     ]
    }
   ],
   "source": [
    "# Create a data loader\n",
    "dataset = torch.utils.data.TensorDataset(X, y)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 4. Training loop\n",
    "\n",
    "train_accuracy_list = []\n",
    "train_loss_list = []\n",
    "\n",
    "#Iterate over the specified number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    #Initialize variables for tracking loss and accuracy\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    #Data is iterated through the data loader in batches\n",
    "    for inputs, targets in dataloader: #batch inputs and targets\n",
    "        #Convert inputs and targets to appropriate data types\n",
    "        inputs = inputs.to(device) #shape: [64, 6] [batch_size, num_inp_features]\n",
    "        targets = targets.to(device) #shape: [64] [batch_size]\n",
    "\n",
    "        #Perform forward pass through the model\n",
    "        outputs = model(inputs) #shape: [64, 2] [batch_size, num_of_classes]\n",
    "        \n",
    "        # Calculate loss\n",
    "        start_pred = outputs[:, 0]\n",
    "        end_pred = outputs[:, 1]\n",
    "        \n",
    "        start_target = targets[:, 0]\n",
    "        end_target = targets[:, 1]\n",
    "        \n",
    "        # Convert the outputs to float tensor\n",
    "        start_pred = start_pred.float()\n",
    "        end_pred = end_pred.float()\n",
    "        \n",
    "        # Convert the targets to long (integer) tensor\n",
    "        start_target = start_target.float()\n",
    "        end_target = end_target.float()\n",
    "        \n",
    "        #Calculate the loss using loss function\n",
    "        start_loss = criterion(start_pred, start_target)\n",
    "        end_loss = criterion(end_pred, end_target)\n",
    "\n",
    "        #Clear the gradients before backward pass, it ensures that the gradients from previous iterations are cleared before calculating new gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Perform a single backward pass by taking average of start step loss and end step loss\n",
    "        loss = (start_loss + end_loss) / 2\n",
    "        loss.backward()\n",
    "        \n",
    "        #Update the neural network parameters based on the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        #Update training loss\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        #Make sure the outputs have appropriate shape and are in the range of [0, 1] using a sigmoid activation function\n",
    "        start_pred = torch.sigmoid(start_pred)\n",
    "        end_pred = torch.sigmoid(end_pred)\n",
    "        \n",
    "        start_pred = start_pred.round()\n",
    "        end_pred = end_pred.round()\n",
    "\n",
    "        #Update training accuracy     \n",
    "        total += start_target.size(0)\n",
    "        total += end_target.size(0)\n",
    "        \n",
    "        correct += (start_pred == start_target).sum().item()  \n",
    "        correct += (end_pred == end_target).sum().item()\n",
    "\n",
    "    #Calculate average training loss\n",
    "    train_loss = train_loss / len(dataset)\n",
    "    \n",
    "    #Calculate average training accuracy\n",
    "    train_accuracy = correct / total\n",
    "    \n",
    "    #Append the training loss and accuracy to seperate lists to plot the graph\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    # Print the training loss and accuracy for the current epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualization\n",
    "\n",
    "#Plot the training accuracy graph\n",
    "plt.plot(range(1, num_epochs+1), train_accuracy_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Accuracy')\n",
    "plt.title('Training Accuracy vs. Epochs')\n",
    "plt.show()\n",
    "\n",
    "#Plot the training loss graph\n",
    "plt.plot(range(1, num_epochs+1), train_loss_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.title('Training Loss vs. Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JtTk1btCTm3"
   },
   "outputs": [],
   "source": [
    "# 6. Test Data and Prediction\n",
    "\n",
    "df_test_data = pd.read_csv(\"testdata.csv\")\n",
    "print(df_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "#Convert the test data to a PyTorch tensor\n",
    "test_inputs = torch.tensor(df_test_data.values, dtype=torch.float64)\n",
    "\n",
    "#Move the test inputs to the specified device\n",
    "test_inputs = test_inputs.to(device)\n",
    "\n",
    "#Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "#Perform forward pass on the test data and obtain predictions for start step and end step probabilities\n",
    "with torch.no_grad():\n",
    "    out = model(test_inputs)\n",
    "    print(out)\n",
    "    start_prob = out[:, 0]\n",
    "    end_prob = out[:, 1]\n",
    "    start_prob = torch.sigmoid(start_prob)\n",
    "    end_prob = torch.sigmoid(end_prob)\n",
    "\n",
    "print(len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pd.DataFrame({'index': range(len(test_inputs)), 'start': start_prob.round(), 'end': end_prob.round()})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "outputs.to_csv('output7_lstm_5.csv', index=False)\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
